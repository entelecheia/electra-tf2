architectures: 
  - ElectraForMaskedLM
attention_probs_dropout_prob: ${training.attention_probs_dropout_prob}
embedding_size: ${training.embedding_size}
hidden_act: ${training.act_func}
hidden_dropout_prob: ${training.hidden_dropout_prob}
hidden_size: ${training.hidden_size}
initializer_range: 0.02
intermediate_size: 1024
layer_norm_eps: 1e-12
max_position_embeddings: ${datamodule.max_seq_length}
model_type: electra
num_attention_heads: 4
num_hidden_layers: 12
pad_token_id: 0
type_vocab_size: 2
vocab_size: ${datamodule.vocab_size}
