# @package _global_

defaults:
  - pretrain_ekon_pretok_base

project: ekonelectra_pretok
training:
  phase2: false
  num_train_steps: 10000
  num_warmup_steps: 2000
  train_batch_size: 176
  max_seq_length: 128
  learning_rate: 6e-3
  gradient_accumulation_steps: 48
  pretrain_tfrecords: ${training.data_dir}/tfrecord_lower_case_1_seq_len_128_random_seed_12345/ekon_pretok/train/pretrain_data*
  wandb_group: ${training.wandb_project}_p1
